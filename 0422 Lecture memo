attention 예제

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras import models, layers

batch_size = 1000
input_size = 32
attention_index = 7

def make_data(batch_size, input_size, attention_index):
    x = np.random,standard_normal((batch,size, input_size))
    y = np.random.randint(low=0, high=2, size=(batch_size, 1))

    x[:, attention_index] = y[:,0]

    return x,y

x = np.random.standard_normal((5,7))

x = np.random.randint(low=0, high=10, size=(5,3))
y = np.random.randint(low=0, high=10, size=(5,3))

x = tf.constant(x)

tf.matmul(x. x, transpose_b=True)

location-base 함수 계산

x = tf.constant([[1,2,3]], dtype = tf.float32)

att = layers.Dense(3, activation = 'softmax', name = 'attention_layer')(x)
layers.Mutiply()[[x. att])

input = tf.keras.Input(shape=(input_size,))

attention_score = layer.Dense(input_size, activation = 'softmax', name = 'attention_layer')(input)
mul = layer.Multiply()([input, attention_score])

x = layers.Dense(64, activation='relu')(mul)
output = layers.Dense(1, activation = 'sigmoid')(x)

model = models.Model(input, output)

tf.keras.utils.plot_model(model)

x, y = make_data(batch_size, input_size, attention_index)

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])
model.fit(x, y, epochs = 100, batch_size = 64, validation_split = 0.2)

