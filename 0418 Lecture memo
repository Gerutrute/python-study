import tensorflow as tf
print(tf.__version__)

import numpy as np
a = np.arrange(10)
a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

np.where(a<5, True // a, False // 10*a)

SimpleRNN을 사용하여 모델 구성하기

from tensorflow.keras.layers import SimpleRNN, Flatten, Dense, Conv1D
from tensorflow.keras.models import Sequential

model = Sequential()

model.add(SimpleRNN(units = 32, activation = 'tanh'. input_shape(n,1))
model.add(dense(1, activation = 'linear'))

model.compile(optimizer = 'adam', loss = 'mse'
model.summary()

Embedding 층을 사용해 단어 임베딩 학습하기

from tensorflow.keras.layers import Embedding

embedding_layer = Embedding(1000,64)

IMDB 데이터셋 사용하기

from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)

one-hot encoding

import numpy as np

def vectorize_sequence(sequences, dimenstion=10000):
    results = np.zeros((len(sequences), dimension))

    for i, sequence in enumerate(sequences) :
        results[i, sequence] = 1
        return results

x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)
y_train = vectorize_sequence(train_labels).astype('float32')
y_test = vectorize_sequence(test_labels).astype('float32')

from keras import models
from keras import layers

model = model.Sequential()
model.add(layers.Dense(16, activation = 'relu', input_shape(10000,))
model.add(layers.Dense(8, activation = 'relu)
model.add(layers.Dense(1, activation = 'sigmoid'))

model.summary()

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['acc'])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[:10000]

history = model.fit(partial_x_train, partial_y_train, epochs=10, batch_size = 512. validation_data = (x_val, y_val))

RNN으로 스팸 메일 분류하기

from tensorflow.keras.layers from SimpleRNN, Embedding, Dense
from tensorflow.keras.models from Sequential

embedding_dim = 32
hidden_units = 32

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(SimpleRNN(hidden_units)
model.add(Dense(1, activation = 'sigmoid)
model.summary()

model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['acc'])
history = model.fit(x_train_padded, y_train, epochs = 4, batch_size=64, validation_split = 0.2)

LSTM으로 IMDB 리뷰 감성 분류하기

from tensorflow.keras.datasets import imdb
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test, y_test) = imdb.load_data()

print(len(X_train))
print(len(X_test))
print(max(y_train_+1))
print(y_train[0])
print(y_train[1])
print(X_train[0])

len_result = [len(s) for s in X_train]
print(np.max(len_result))
print(np.mean(len_result)) 최대길이 평균길이 차이남.

from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)

max_len = 500
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

model = sequential()
model.add(Embedding(5000, 120))
model.add(LSTM(120))
model.add(Dense, activation = 'sigmoid'))

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)

