Sequential API
- tensor가 이전 layer에서 다음 layer로 바로 이어질 때에 사용 가능.
- 장점 : 직관적이고 편리함.
- 단점 : 다수의 입력(multi-input), 다수의 출력(multi-output)을 가진 모델 또는 층 간의 연결(concatenate)이나 덧셈(Add)과 같은 연산을 하는 모델을 구현하기에는 적합하지 않습니다.

import tensorflow as tf

tf.random.set_seed(777)
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import mse

x = np.array([[0,0],[1,0],[0,1],[1,1]])
y = np.array([[0],[1],[1],[0]])

model = Sequential()

model.add(Dense, input_shape = (2, ), activation = 'linear')

model.compile(optimizer = 'SGD',
    loss = mse,
    metrics = ['acc'])

model.fit(x, y, epochs = 500)

모델 시각화
from tensorflow.keras.utils import plot_model
plot_model(model, show_shapes=True)

Functional API 모델 생성

import tensorflow as tf
import numpy as np

x_data = [[0.,0.], [0.,1.], [1.,0.],[1.,1.]]
y_data = [[0.], [1.], [1.], [0.]]

input_Layer = tf.keras.layers.Input(shape = (2,))
x = tf.keras.layers.Dense(2, activation = 'sigmoid')(input_Layer)
Out_Layer = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)
model = tf.keras.Model(input=[input_Layer], outputs=[Out_Layer])
model.summary()

optimizer = tf.keras.optimizers.SGD(lr=0.7)
loss=tf.keras.losses.binary_crossentroy
metrics=tf.keras.metrics.binary_accuracy

model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])

model.fit(x_data, y_data, epochs=100, batch_size=4)

모델 구축

input_A = keras.layers.Input(shape=[5], name='wide_input")
input_B = keras.layers.Input(shape=[6], name='deep_input")
hidden1= keras.layers.Dense(30, activation = 'relu')(input_B)
hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)
model = keras.Model(inputs=[input_A, input_B], outputs=[output])

tf.keras.Layer를 상속받아 custom layer 만들기

class MyLayer(tf.keras.layers.Layer):
    def __init__(self):
        self.d1 = tf.keras.layers.Dense(64, activation = 'relu')
        self.d2 = tf.keras.layers.Dense(10, activation = 'softmax')

    def call(self, x):
        x = self.d1(x)
        return self.d2(x)

inputs = tf.keras.Input(shape=(784,))

outputs = MyLayer()(inputs)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name = "mnist_model")

MNIST Dataset with perceptron
- 0~9로 이루어진 데이터 셋
- 28 * 28 픽셀, 6만장의 데이터


import tensorflow as tf

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

import matplotlib.pyplot as plt
import numpy as np

sample_size = 10

random_idx = np.random.randint(60000, size = sample_size)

print(random_idx)

for idx in random_idx:
    img = X_train[idx, :]
    label = y_train[idx]
    plt.figure()
    plt.imshow(img)
    plt.title('%d-th data, label is %d' %(idx, label))

# 프로젝트에서는 라벨 붙이는 작업이 제일 힘듦

검증데이터 만들기

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 777)

print(f'훈련 데이터 {X_train.shape} 레이블 {y_train.shape}')
print(f'검증 데이터 {X_val.shape} 레이블 {y_val.shape}')

정규분포 만들기
2차원인 숫자 사진을 딥러닝 차원인 1차원으로 변환

num_X_train = X_train.shape[0]
num_X_val = X_val.shape[0]
num_X_test = X_test.shape[0]

X_train = (X_train.reshape((num_X_train, 28*28))) / 255

X_val = (X_val.reshape((num_X_val, 28*28))) / 255
X_test = (X_test.reshape((num_X_test, 28*28))) / 255

print(X_train.shape)

one-hot encoding 답 비교

from tensorflow.keras.utils import to_categorical

num_classes = 10

y_train = to_categorical(y_train, num_classes)
y_val = to_categorical(y_val, num_classes)
y_test = to_categorical(y_test, num_classes)

모델 구성

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

model.add(Dense(64, activation = 'relu', input_shape = (784,)))
model.add(Dense(32, activation = 'relu'))
model.add(Dense(10, activation = 'softmax'))
model.summary()

학습과정 설정하기
model.compile(optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics = ['acc'])

history = model.fit(X_train, y_train,
    epochs = 30,
    batch_size=128,
    validation_data = (X_val, y_val),
    verbose=1)

imdb 이진 분류

from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

max([max(sequence) for sequence in train_data])

import numpy as np
def vectorize_sequence(sequences, dimension = 10000):
    result = np.zeros((len(sequences), dimension))
    for i, sequences in enumerate(sequences):
        result[i, sequences] = 1.0
    return result

X_train = vectorize_sequences(train_data)
X_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype(float)
y_test = np.asarray(test_labels).astype(float)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

imdb = Sequential()

imdb.add(Dense(64, activation = 'relu', input_shape = (10000,)))
imdb.add(Dense(32, activation = 'relu'))
imdb.add(Dense(1, activation = 'sigmoid'))
imdb.summary()

imdb.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])

X_valid = X_train[:10000]
partial_X_train = X_train[10000:]
y_valid = y_train[:10000]
partial_y_train = y_train[10000:]

hist_imb = imdb.fit(partial_X_train, partial_y_train, epochs=50, batch_size=512l validation_data=(X_valid, y_valid))

